{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde193fa",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è Multimodal Processing with Amazon S3 Vectors Memory\n",
    "\n",
    "This notebook demonstrates multi-modal content processing using **Strands Agents** with **Amazon S3 Vectors** as the memory backend. This is the production-ready episode in our multi-modal AI processing series.\n",
    "\n",
    "![s3](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/07/16/2025-s3-vector-1-vector-overview-1.png)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **AWS-Native Memory**: Use Amazon S3 Vectors for scalable memory storage\n",
    "- **Multi-Modal Processing**: Analyze images, documents, and videos with persistent memory\n",
    "- **Conversation Continuity**: Maintain context across sessions with automatic memory\n",
    "- **Production Ready**: Enterprise-grade memory solution with AWS integration\n",
    "\n",
    "## Series Context\n",
    "\n",
    "This builds upon our previous episode: [Multi-Modal Content Processing with FAISS Memory](https://dev.to/aws/multi-modal-content-processing-with-strands-agent-and-faiss-memory-39hg)\n",
    "\n",
    "**Key Upgrade**: Moving from local FAISS to AWS-native Amazon S3 Vectors for production-ready memory management.\n",
    "\n",
    "![a2a](../image/s3_memory.png)\n",
    "\n",
    "## From FAISS to S3 Vectors\n",
    "\n",
    "**FAISS (Chapter 3)**\n",
    "- ‚úÖ Great for development\n",
    "- ‚úÖ Fast local testing\n",
    "- ‚ùå Single machine only\n",
    "- ‚ùå No built-in durability\n",
    "\n",
    "**S3 Vectors (Chapter 4)**\n",
    "- ‚úÖ AWS-managed service\n",
    "- ‚úÖ Scales automatically\n",
    "- ‚úÖ Built-in durability\n",
    "- ‚úÖ Multi-user isolation\n",
    "\n",
    "Let's upgrade our agent to production-ready memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4559fc",
   "metadata": {},
   "source": [
    "## ü§ñ Agent Configuration with S3 Vectors Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from strands.models import BedrockModel\n",
    "from strands import Agent\n",
    "from strands_tools import image_reader, file_read, use_llm\n",
    "from video_reader import video_reader\n",
    "from s3_memory import s3_vector_memory  # Our new S3 Vectors memory tool\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "## Setup S3 Vectors\n",
    "\n",
    "Configure your bucket and index names. The tool will create them automatically on first use if they don't exist.\n",
    "\n",
    "**What gets created automatically:**\n",
    "- ‚úÖ S3 Vector bucket (if it doesn't exist)\n",
    "- ‚úÖ Vector index with 1024 dimensions (Nova/Titan compatible)\n",
    "- ‚úÖ Cosine similarity metric\n",
    "- ‚úÖ User isolation via metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 Vectors (bucket and index will be created automatically if they don't exist)\n",
    "# ‚ö†Ô∏è CHANGE THIS!\n",
    "os.environ['VECTOR_BUCKET_NAME'] = 'YOUR-INDEX-NAME'  # Choose your bucket name \n",
    "# ‚ö†Ô∏è CHANGE THIS!\n",
    "os.environ['VECTOR_INDEX_NAME'] = 'YOU-BUCKET-NAME'        # Choose your index name  \n",
    "os.environ['AWS_REGION'] = 'us-east-1'                       # Your AWS region\n",
    "os.environ['EMBEDDING_MODEL'] = 'amazon.nova-2-multimodal-embeddings-v1:0'  # Nova embeddings\n",
    "\n",
    "USER_ID = \"demo_user\"  # Your user ID for memory isolation\n",
    "\n",
    "print(f\"‚úÖ Config set - bucket and index will be created on first use\")\n",
    "print(f\"   Bucket: {os.environ['VECTOR_BUCKET_NAME']}\")\n",
    "print(f\"   Index: {os.environ['VECTOR_INDEX_NAME']}\")\n",
    "print(f\"   User: {USER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daf993",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "agent-section",
   "metadata": {},
   "source": [
    "## Create Agent with S3 Memory\n",
    "\n",
    "Same agent as Chapter 3, but now using `s3_vector_memory` instead of `mem0_memory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    region=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# System prompt for multi-modal processing with memory\n",
    "MULTIMODAL_SYSTEM_PROMPT = \"\"\"You are an AI assistant with multi-modal processing capabilities and persistent memory.\n",
    "\n",
    "Your capabilities:\n",
    "- **Multi-Modal Analysis**: Process images, documents, videos, and text\n",
    "- **Persistent Memory**: Remember preferences, previous analyses, and conversation history\n",
    "- **Context Awareness**: Use memory to provide personalized and contextual responses\n",
    "- **Continuous Learning**: Build understanding over time through memory accumulation\n",
    "\n",
    "Memory Usage Guidelines:\n",
    "- Check for relevant memories before responding\n",
    "- Store important insights, preferences, and analysis results\n",
    "- Reference previous conversations when relevant\n",
    "- Maintain conversation continuity across sessions\n",
    "\n",
    "When processing content:\n",
    "1. First retrieve relevant memories for context\n",
    "2. Analyze the new content thoroughly\n",
    "3. Store key insights and findings\n",
    "4. Provide comprehensive responses using both new analysis and memory context\n",
    "\"\"\"\n",
    "\n",
    "# Create the multi-modal agent with S3 Vectors memory\n",
    "multimodal_agent = Agent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        s3_vector_memory,  # Our S3 Vectors memory tool\n",
    "        image_reader,      # Image processing\n",
    "        file_read,         # Document processing  \n",
    "        video_reader,      # Video processing\n",
    "        use_llm           # Advanced reasoning\n",
    "    ],\n",
    "    system_prompt=MULTIMODAL_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"Multi-modal agent with S3 Vectors memory created successfully!\")\n",
    "print(\"Memory backend: Amazon S3 Vectors\")\n",
    "print(\"Tools loaded: S3 Memory, Image Reader, File Reader, Video Reader, LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Simulate first interaction - establishing preferences\n",
    "\n",
    "response1 = multimodal_agent(\n",
    "    f\"\"\"Hello, I'm Elizabeth Fuentes. You can call me Eli, I'm a developer advocate at AWS, I like to work early in the morning, \n",
    "    I prefer Italian coffee, and I want to understand what's in images, videos, and documents to improve my day-to-day work. \n",
    "    I'm also very interested in artificial intelligence and work in the financial sector.\n",
    "    \n",
    "    Please save this information about my preferences for future conversations.\n",
    "    \n",
    "    USER_ID: {USER_ID}\"\"\"\n",
    ")\n",
    "\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-test-section",
   "metadata": {},
   "source": [
    "## Test S3 Memory\n",
    "\n",
    "Quick test - same API as FAISS, different backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a memory\n",
    "result = s3_vector_memory(\n",
    "    action=\"store\",\n",
    "    content=\"User prefers AWS architecture and serverless solutions\",\n",
    "    user_id=USER_ID\n",
    ")\n",
    "print(f\"Store: {result['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve memories\n",
    "result = s3_vector_memory(\n",
    "    action=\"retrieve\",\n",
    "    query=\"AWS preferences\",\n",
    "    user_id=USER_ID\n",
    ")\n",
    "print(f\"Retrieve: {result['total_found']} memories found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f295665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all memories\n",
    "memory_result = s3_vector_memory(\n",
    "    action=\"list\",\n",
    "    user_id=USER_ID\n",
    ")\n",
    "print(f\"Total memories in system: {memory_result['total_found']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_result['memories']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "image-section",
   "metadata": {},
   "source": [
    "## Use It - Same as Chapter 3\n",
    "\n",
    "The agent works exactly like Chapter 3, but memory is now in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze architectural diagram with memory context\n",
    "print(\"Analyzing Architectural Diagram with Memory Context...\\n\")\n",
    "\n",
    "image_response = multimodal_agent(\n",
    "    f\"\"\"Analyze the architectural diagram in data-sample/diagram.jpg. \n",
    "    \n",
    "    Before analyzing:\n",
    "    1. Check my memory for any previous architectural discussions or preferences\n",
    "    2. Use that context to provide a more personalized analysis\n",
    "    \n",
    "    After analysis:\n",
    "    1. Store the key architectural insights you discovered\n",
    "    2. Note any patterns or technologies that align with my interests\n",
    "    \n",
    "    My user ID for memory operations: {USER_ID}\n",
    "    \n",
    "    Provide a comprehensive analysis including:\n",
    "    - Architecture overview and components\n",
    "    - Technology stack identification\n",
    "    - Best practices observed\n",
    "    - Recommendations based on my preferences\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Image Analysis Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(image_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-section",
   "metadata": {},
   "source": [
    "## Document Processing with Memory\n",
    "\n",
    "Process PDF documents while maintaining conversation context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "document-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process AWS documentation with memory integration\n",
    "print(\"Processing AWS Documentation with Memory Integration...\\n\")\n",
    "\n",
    "document_response = multimodal_agent(\n",
    "    f\"\"\"Process the document data-sample/Welcome-Strands-Agents-SDK.pdf.\n",
    "    \n",
    "    Memory-enhanced processing:\n",
    "    1. First, retrieve any relevant memories about my interests in AWS, AI, or development tools\n",
    "    2. Process the document with that context in mind\n",
    "    3. Store key insights that relate to my interests\n",
    "    4. Connect the document content to our previous architectural discussion\n",
    "    \n",
    "    My user ID: {USER_ID}\n",
    "    \n",
    "    Focus on:\n",
    "    - How Strands Agents relates to the architecture we analyzed\n",
    "    - Key features that would interest someone focused on AWS and serverless\n",
    "    - Practical applications for multi-modal AI processing\n",
    "    - Integration possibilities with AWS services\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Document Processing Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(document_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "video-section",
   "metadata": {},
   "source": [
    "## Video Analysis with Memory\n",
    "\n",
    "Process video content with full memory context from previous analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "video-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze video with comprehensive memory context\n",
    "print(\"Analyzing Video with Comprehensive Memory Context...\\n\")\n",
    "\n",
    "video_response = multimodal_agent(\n",
    "    f\"\"\"Analyze the video data-sample/moderation-video.mp4 using our full conversation history.\n",
    "    \n",
    "    Memory-driven analysis:\n",
    "    1. Retrieve all relevant memories from our session (architecture, documents, preferences)\n",
    "    2. Analyze the video content in the context of our previous discussions\n",
    "    3. Store insights about video content moderation and AI applications\n",
    "    4. Connect this to the broader multi-modal AI processing theme\n",
    "    \n",
    "    My user ID: {USER_ID}\n",
    "    \n",
    "    Comprehensive analysis should include:\n",
    "    - Video content summary and key scenes\n",
    "    - Technical aspects related to content moderation\n",
    "    - How this relates to our architectural and Strands Agent discussions\n",
    "    - Practical applications in AWS/serverless environments\n",
    "    - Integration possibilities with the technologies we've discussed\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Video Analysis Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(video_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthesis-section",
   "metadata": {},
   "source": [
    "## Memory-Driven Synthesis\n",
    "\n",
    "Create a comprehensive synthesis using all stored memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive synthesis using all memories\n",
    "print(\"Generating Memory-Driven Synthesis...\\n\")\n",
    "\n",
    "synthesis_response = multimodal_agent(\n",
    "    f\"\"\"Create a comprehensive synthesis of our entire multi-modal processing session.\n",
    "    \n",
    "    Memory synthesis process:\n",
    "    1. Retrieve ALL memories from our session using my user ID: {USER_ID}\n",
    "    2. Analyze patterns and connections across all processed content\n",
    "    3. Store this synthesis as a session summary\n",
    "    4. Provide actionable insights and recommendations\n",
    "    \n",
    "    Your synthesis should cover:\n",
    "    - **Architecture Insights**: Key findings from the diagram analysis\n",
    "    - **Technology Stack**: Strands Agents capabilities and AWS integration\n",
    "    - **Multi-Modal Applications**: Practical use cases we've explored\n",
    "    - **Content Moderation**: Video analysis insights and applications\n",
    "    - **Memory Benefits**: How S3 Vectors memory enhanced our analysis\n",
    "    - **Next Steps**: Recommendations for implementing these solutions\n",
    "    \n",
    "    Make this a comprehensive technical summary that demonstrates the power of \n",
    "    persistent memory in multi-modal AI processing.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Comprehensive Synthesis Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(synthesis_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-continuity-section",
   "metadata": {},
   "source": [
    "## Conversation Continuity Test\n",
    "\n",
    "Test how memory provides conversation continuity across sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuity-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a new conversation session with memory context\n",
    "print(\"Testing Conversation Continuity with S3 Vectors Memory\\n\")\n",
    "\n",
    "continuity_response = multimodal_agent(\n",
    "    f\"\"\"Hi! I'm back for a new session. Can you remind me what we discussed previously \n",
    "    and provide some follow-up recommendations based on our multi-modal analysis?\n",
    "    \n",
    "    Use my memory (user ID: {USER_ID}) to:\n",
    "    1. Summarize our previous session\n",
    "    2. Highlight key insights we discovered\n",
    "    3. Suggest next steps for implementation\n",
    "    4. Recommend additional AWS services that would complement our findings\n",
    "    \n",
    "    This demonstrates the power of persistent memory in AI conversations!\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Conversation Continuity Test Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(continuity_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a7079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

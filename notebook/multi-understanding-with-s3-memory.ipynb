{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde193fa",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è Multi-Agent Multimodal Analysis with S3 Vectors Memory\n",
    "\n",
    "This notebook demonstrates multi-modal content processing using **Strands Agents** with **Amazon S3 Vectors** as the memory backend. This is the production-ready episode in our multi-modal AI processing series.\n",
    "\n",
    "![s3](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/07/16/2025-s3-vector-1-vector-overview-1.png)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **AWS-Native Memory**: Use Amazon S3 Vectors for scalable memory storage\n",
    "- **Multi-Modal Processing**: Analyze images, documents, and videos with persistent memory\n",
    "- **Conversation Continuity**: Maintain context across sessions with automatic memory\n",
    "- **Production Ready**: Enterprise-grade memory solution with AWS integration\n",
    "\n",
    "## Series Context\n",
    "\n",
    "This builds upon our previous episode: [Multi-Modal Content Processing with FAISS Memory](https://dev.to/aws/multi-modal-content-processing-with-strands-agent-and-faiss-memory-39hg)\n",
    "\n",
    "**Key Upgrade**: Moving from local FAISS to AWS-native Amazon S3 Vectors for production-ready memory management.\n",
    "\n",
    "![a2a](image/s3_memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4559fc",
   "metadata": {},
   "source": [
    "## ü§ñ Agent Configuration with S3 Vectors Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from strands.models import BedrockModel\n",
    "from strands import Agent\n",
    "from strands_tools import image_reader, file_read, use_llm\n",
    "from video_reader import video_reader\n",
    "from s3_memory import s3_vector_memory  # Our new S3 Vectors memory tool\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "### üîß Environment Configuration\n",
    "\n",
    "Configure your Amazon S3 Vectors and AWS settings:\n",
    "\n",
    "[Tutorial: Getting started with S3 Vectors](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-vectors-getting-started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Vectors Configuration\n",
    "os.environ['VECTOR_BUCKET_NAME'] = 'YOUR-S3-BUCKET'  # Your S3 Vector bucket\n",
    "os.environ['VECTOR_INDEX_NAME'] = 'YOUR-VECTOR-INDEX'        # Your vector index\n",
    "os.environ['AWS_REGION'] = 'us-east-1'                       # AWS region\n",
    "os.environ['EMBEDDING_MODEL'] = 'amazon.titan-embed-text-v2:0' # Bedrock embedding model\n",
    "\n",
    "# User identification for memory isolation\n",
    "USER_ID = \"demo_user_s3_vectors\"\n",
    "\n",
    "print(f\"Configuration set for user: {USER_ID}\")\n",
    "print(f\"S3 Vector bucket: {os.environ['VECTOR_BUCKET_NAME']}\")\n",
    "print(f\"Vector index: {os.environ['VECTOR_INDEX_NAME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-section",
   "metadata": {},
   "source": [
    "### üöÄ Agent Setup with S3 Vectors Memory\n",
    "\n",
    "Create our multi-modal agent with AWS-native memory capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    region=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# System prompt for multi-modal processing with memory\n",
    "MULTIMODAL_SYSTEM_PROMPT = \"\"\"You are an AI assistant with multi-modal processing capabilities and persistent memory.\n",
    "\n",
    "Your capabilities:\n",
    "- **Multi-Modal Analysis**: Process images, documents, videos, and text\n",
    "- **Persistent Memory**: Remember preferences, previous analyses, and conversation history\n",
    "- **Context Awareness**: Use memory to provide personalized and contextual responses\n",
    "- **Continuous Learning**: Build understanding over time through memory accumulation\n",
    "\n",
    "Memory Usage Guidelines:\n",
    "- Check for relevant memories before responding\n",
    "- Store important insights, preferences, and analysis results\n",
    "- Reference previous conversations when relevant\n",
    "- Maintain conversation continuity across sessions\n",
    "\n",
    "When processing content:\n",
    "1. First retrieve relevant memories for context\n",
    "2. Analyze the new content thoroughly\n",
    "3. Store key insights and findings\n",
    "4. Provide comprehensive responses using both new analysis and memory context\n",
    "\"\"\"\n",
    "\n",
    "# Create the multi-modal agent with S3 Vectors memory\n",
    "multimodal_agent = Agent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        s3_vector_memory,  # Our S3 Vectors memory tool\n",
    "        image_reader,      # Image processing\n",
    "        file_read,         # Document processing  \n",
    "        video_reader,      # Video processing\n",
    "        use_llm           # Advanced reasoning\n",
    "    ],\n",
    "    system_prompt=MULTIMODAL_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "print(\"Multi-modal agent with S3 Vectors memory created successfully!\")\n",
    "print(\"Memory backend: Amazon S3 Vectors\")\n",
    "print(\"Tools loaded: S3 Memory, Image Reader, File Reader, Video Reader, LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Simulate first interaction - establishing preferences\n",
    "\n",
    "response1 = multimodal_agent(\n",
    "    f\"\"\"Hello, I'm Elizabeth Fuentes. You can call me Eli, I'm a developer advocate at AWS, I like to work early in the morning, \n",
    "    I prefer Italian coffee, and I want to understand what's in images, videos, and documents to improve my day-to-day work. \n",
    "    I'm also very interested in artificial intelligence and work in the financial sector.\n",
    "    \n",
    "    Please save this information about my preferences for future conversations.\n",
    "    \n",
    "    USER_ID: {USER_ID}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-test-section",
   "metadata": {},
   "source": [
    "## Memory System Testing\n",
    "\n",
    "Test our S3 Vectors memory system before processing multi-modal content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory storage\n",
    "print(\"Testing S3 Vectors Memory System...\\n\")\n",
    "\n",
    "# Store initial preferences\n",
    "memory_result = s3_vector_memory(\n",
    "    action=\"store\",\n",
    "    content=\"You are interested in AWS architecture, serverless solutions, and multi-modal AI processing. You prefer detailed technical explanations.\",\n",
    "    user_id=USER_ID\n",
    ")\n",
    "print(f\"Stored preferences: {memory_result['status']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory retrieval\n",
    "memory_result = s3_vector_memory(\n",
    "    action=\"retrieve\",\n",
    "    query=\"preferences and interests\",\n",
    "    user_id=USER_ID\n",
    ")\n",
    "print(f\"Retrieved memories: {memory_result['total_found']} found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f295665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List all memories\n",
    "memory_result = s3_vector_memory(\n",
    "    action=\"list\",\n",
    "    user_id=USER_ID\n",
    ")\n",
    "print(f\"Total memories in system: {memory_result['total_found']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "image-section",
   "metadata": {},
   "source": [
    "## Image Analysis with Memory\n",
    "\n",
    "Process images with persistent memory for context and insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze architectural diagram with memory context\n",
    "print(\"Analyzing Architectural Diagram with Memory Context...\\n\")\n",
    "\n",
    "image_response = multimodal_agent(\n",
    "    f\"\"\"Analyze the architectural diagram in data-sample/diagram.jpg. \n",
    "    \n",
    "    Before analyzing:\n",
    "    1. Check my memory for any previous architectural discussions or preferences\n",
    "    2. Use that context to provide a more personalized analysis\n",
    "    \n",
    "    After analysis:\n",
    "    1. Store the key architectural insights you discovered\n",
    "    2. Note any patterns or technologies that align with my interests\n",
    "    \n",
    "    My user ID for memory operations: {USER_ID}\n",
    "    \n",
    "    Provide a comprehensive analysis including:\n",
    "    - Architecture overview and components\n",
    "    - Technology stack identification\n",
    "    - Best practices observed\n",
    "    - Recommendations based on my preferences\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Image Analysis Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(image_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "document-section",
   "metadata": {},
   "source": [
    "## Document Processing with Memory\n",
    "\n",
    "Process PDF documents while maintaining conversation context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "document-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process AWS documentation with memory integration\n",
    "print(\"Processing AWS Documentation with Memory Integration...\\n\")\n",
    "\n",
    "document_response = multimodal_agent(\n",
    "    f\"\"\"Process the document data-sample/Welcome-Strands-Agents-SDK.pdf.\n",
    "    \n",
    "    Memory-enhanced processing:\n",
    "    1. First, retrieve any relevant memories about my interests in AWS, AI, or development tools\n",
    "    2. Process the document with that context in mind\n",
    "    3. Store key insights that relate to my interests\n",
    "    4. Connect the document content to our previous architectural discussion\n",
    "    \n",
    "    My user ID: {USER_ID}\n",
    "    \n",
    "    Focus on:\n",
    "    - How Strands Agents relates to the architecture we analyzed\n",
    "    - Key features that would interest someone focused on AWS and serverless\n",
    "    - Practical applications for multi-modal AI processing\n",
    "    - Integration possibilities with AWS services\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Document Processing Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(document_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "video-section",
   "metadata": {},
   "source": [
    "## Video Analysis with Memory\n",
    "\n",
    "Process video content with full memory context from previous analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "video-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze video with comprehensive memory context\n",
    "print(\"Analyzing Video with Comprehensive Memory Context...\\n\")\n",
    "\n",
    "video_response = multimodal_agent(\n",
    "    f\"\"\"Analyze the video data-sample/moderation-video.mp4 using our full conversation history.\n",
    "    \n",
    "    Memory-driven analysis:\n",
    "    1. Retrieve all relevant memories from our session (architecture, documents, preferences)\n",
    "    2. Analyze the video content in the context of our previous discussions\n",
    "    3. Store insights about video content moderation and AI applications\n",
    "    4. Connect this to the broader multi-modal AI processing theme\n",
    "    \n",
    "    My user ID: {USER_ID}\n",
    "    \n",
    "    Comprehensive analysis should include:\n",
    "    - Video content summary and key scenes\n",
    "    - Technical aspects related to content moderation\n",
    "    - How this relates to our architectural and Strands Agent discussions\n",
    "    - Practical applications in AWS/serverless environments\n",
    "    - Integration possibilities with the technologies we've discussed\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Video Analysis Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(video_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthesis-section",
   "metadata": {},
   "source": [
    "## Memory-Driven Synthesis\n",
    "\n",
    "Create a comprehensive synthesis using all stored memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive synthesis using all memories\n",
    "print(\"Generating Memory-Driven Synthesis...\\n\")\n",
    "\n",
    "synthesis_response = multimodal_agent(\n",
    "    f\"\"\"Create a comprehensive synthesis of our entire multi-modal processing session.\n",
    "    \n",
    "    Memory synthesis process:\n",
    "    1. Retrieve ALL memories from our session using my user ID: {USER_ID}\n",
    "    2. Analyze patterns and connections across all processed content\n",
    "    3. Store this synthesis as a session summary\n",
    "    4. Provide actionable insights and recommendations\n",
    "    \n",
    "    Your synthesis should cover:\n",
    "    - **Architecture Insights**: Key findings from the diagram analysis\n",
    "    - **Technology Stack**: Strands Agents capabilities and AWS integration\n",
    "    - **Multi-Modal Applications**: Practical use cases we've explored\n",
    "    - **Content Moderation**: Video analysis insights and applications\n",
    "    - **Memory Benefits**: How S3 Vectors memory enhanced our analysis\n",
    "    - **Next Steps**: Recommendations for implementing these solutions\n",
    "    \n",
    "    Make this a comprehensive technical summary that demonstrates the power of \n",
    "    persistent memory in multi-modal AI processing.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Comprehensive Synthesis Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(synthesis_response.message)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-review-section",
   "metadata": {},
   "source": [
    "## Memory System Review\n",
    "\n",
    "Review what our S3 Vectors memory system has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review all stored memories\n",
    "print(\"S3 Vectors Memory System Review\\n\")\n",
    "\n",
    "# Get comprehensive memory list\n",
    "all_memories = s3_vector_memory(\n",
    "    action=\"list\",\n",
    "    user_id=USER_ID,\n",
    "    top_k=50  # Get more memories\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal memories stored: {all_memories['total_found']}\")\n",
    "print(\"\\nMemory Categories Learned:\")\n",
    "\n",
    "# Test different memory retrievals\n",
    "categories = [\n",
    "    \"architecture and system design\",\n",
    "    \"Strands Agents and AI tools\", \n",
    "    \"video content and moderation\",\n",
    "    \"preferences and interests\",\n",
    "    \"AWS and serverless technologies\"\n",
    "]\n",
    "\n",
    "for category in categories:\n",
    "    result = s3_vector_memory(\n",
    "        action=\"retrieve\",\n",
    "        query=category,\n",
    "        user_id=USER_ID,\n",
    "        top_k=3\n",
    "    )\n",
    "    print(f\"  {category.title()}: {result['total_found']} relevant memories\")\n",
    "\n",
    "print(\"\\nMemory system successfully captured multi-modal processing insights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-continuity-section",
   "metadata": {},
   "source": [
    "## Conversation Continuity Test\n",
    "\n",
    "Test how memory provides conversation continuity across sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuity-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a new conversation session with memory context\n",
    "print(\"Testing Conversation Continuity with S3 Vectors Memory\\n\")\n",
    "\n",
    "continuity_response = multimodal_agent(\n",
    "    f\"\"\"Hi! I'm back for a new session. Can you remind me what we discussed previously \n",
    "    and provide some follow-up recommendations based on our multi-modal analysis?\n",
    "    \n",
    "    Use my memory (user ID: {USER_ID}) to:\n",
    "    1. Summarize our previous session\n",
    "    2. Highlight key insights we discovered\n",
    "    3. Suggest next steps for implementation\n",
    "    4. Recommend additional AWS services that would complement our findings\n",
    "    \n",
    "    This demonstrates the power of persistent memory in AI conversations!\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Conversation Continuity Test Complete!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(continuity_response.message)\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

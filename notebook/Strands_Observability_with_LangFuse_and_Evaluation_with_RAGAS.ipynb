{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability with LangFuse and Evaluation with RAGAS üîçüìä\n",
    "\n",
    "In the Strands Agents SDK, observability refers to your ability to measure system behavior and performance. Observability combines instrumentation, data collection, and analysis techniques. These techniques provide insights into an agent's behavior and performance, helping you effectively build, debug, and maintain agents that better serve your unique needs and reliably complete tasks.\n",
    "\n",
    "This notebook demonstrates how to build an agent with observability and evaluation capabilities. \n",
    "\n",
    "We use [Langfuse](https://langfuse.com/) to process the Strands Agent traces and [Ragas](https://www.ragas.io/) metrics to evaluate agent performance. The primary focus is on agent evaluation and the quality of responses generated by the agent using traces produced by the SDK.\n",
    "\n",
    "### What is Observability and Evaluation?\n",
    "\n",
    "**Observability** means being able to see what your AI agent is doing \"behind the scenes\" - like watching its thought process. It helps you understand why your agent makes certain decisions or gives particular responses.\n",
    "\n",
    "**Evaluation** is how we measure if our agent is doing a good job. Instead of just guessing if responses are good, we use specific metrics to score the agent's performance.\n",
    "\n",
    "### Observability Components\n",
    "\n",
    "All observability APIs are embedded directly within the Strands Agents SDK. The following are key observability data points:\n",
    "\n",
    "[**Metrics**](https://strandsagents.com/latest/user-guide/observability-evaluation/metrics/) - Essential for understanding agent performance, optimizing behavior, and monitoring resource usage.\n",
    "\n",
    "[**Traces**](https://strandsagents.com/latest/user-guide/observability-evaluation/traces/) - A fundamental component of the Strands SDK's observability framework, providing detailed insights into your agent's execution.\n",
    "\n",
    "[**Logs**](https://strandsagents.com/latest/user-guide/observability-evaluation/logs/) - Strands SDK uses Python's standard logging module to provide visibility into operations.\n",
    "\n",
    "[**Evaluation**](https://strandsagents.com/latest/user-guide/observability-evaluation/evaluation/) - Essential for measuring agent performance, tracking improvements, and ensuring your agents meet quality standards. With Strands SDK, you can perform Manual Evaluation, Structured Testing, LLM Judge Evaluation, and Tool-Specific Evaluation.\n",
    "\n",
    "### OpenTelemetry Integration\n",
    "\n",
    "Strands natively integrates with OpenTelemetry, an industry standard for distributed tracing. You can visualize and analyze traces using any OpenTelemetry-compatible tool. This integration provides:\n",
    "\n",
    "- **Compatibility with existing observability tools:** Send traces to platforms such as Jaeger, Grafana Tempo, AWS X-Ray, Datadog, and more\n",
    "- **Standardized attribute naming:** Uses OpenTelemetry semantic conventions\n",
    "- **Flexible export options:** Console output for development, OTLP endpoint for production\n",
    "- **Auto-instrumentation:** Trace creation is handled automatically when you turn on tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üçΩÔ∏èüîçObservability and Evaluation with Restaurant Agent\n",
    "\n",
    "In this notebook, we'll demonstrate how to build a restaurant recommendation agent with observability and evaluation capabilities. This is designed for beginners who want to learn about AI agents, observability, and evaluation without complex infrastructure setup.\n",
    "\n",
    "> ‚≠ê Based on the code from [08-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb](https://github.com/strands-agents/samples/blob/main/01-tutorials/01-fundamentals/08-observability-and-evaluation/Observability-and-Evaluation-sample.ipynb) of the [Strands Agents Samples repository](https://github.com/strands-agents/)\n",
    "\n",
    "### What We'll Build\n",
    "\n",
    "We'll use these key components:\n",
    "\n",
    "1. **Local Vector Database**: A searchable collection of restaurant information that our agent can query\n",
    "2. **Strands Agent**: An AI assistant that can recommend restaurants based on user preferences\n",
    "3. **LangFuse**: A tool that lets us \"see\" how our agent works and makes decisions\n",
    "4. **RAGAS**: A framework that helps us evaluate how well our agent is performing\n",

    "![image](image/restaurant_agent_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Install Required Packages\n",
    "\n",
    "First, we need to install all the necessary packages for our notebook. Each package has a specific purpose:\n",
    "\n",
    "- **langchain**: Helps us build applications with language models\n",
    "- **langfuse**: Provides observability for our agent\n",
    "- **ragas**: Helps us evaluate our agent's performance\n",
    "- **chromadb**: A database for storing and searching vector embeddings\n",
    "- **docx2txt**: Converts Word documents to text\n",
    "- **boto3**: AWS SDK for Python, used to access AWS services\n",
    "- **strands**: Framework for building AI agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# The -q flag makes the installation quieter (less output)\n",
    "!pip install -q langchain langfuse ragas chromadb docx2txt boto3 strands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Create Vector Database from Restaurant Data\n",
    "\n",
    "A vector database stores text as numbers (vectors) that represent the meaning of the text. This allows us to search for similar meanings, not just exact word matches. For example, if we search for \"vegetarian food\", we might also find results about \"plant-based dishes\" even if those exact words weren't used.\n",
    "\n",
    "We'll create a vector database using restaurant data files in the `restaurant-data` folder. These files contain information about different restaurants, their menus, and specialties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for document loading\n",
    "import os  # For working with files and directories\n",
    "import docx2txt  # For converting Word documents to text\n",
    "from langchain.document_loaders import TextLoader  # For loading text documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into chunks\n",
    "from langchain.schema.document import Document  # For creating document objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load DOCX files\n",
    "def load_docx(file_path):\n",
    "    \"\"\"\n",
    "    This function takes a Word document (.docx) file path and converts it to text.\n",
    "    It then creates a Document object that can be used by our vector database.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the DOCX file\n",
    "        \n",
    "    Returns:\n",
    "        A Document object containing the text and metadata\n",
    "    \"\"\"\n",
    "    # Extract text from the DOCX file\n",
    "    text = docx2txt.process(file_path)\n",
    "    \n",
    "    # Create a Document object with the text and source information\n",
    "    return Document(page_content=text, metadata={\"source\": file_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all restaurant data files\n",
    "restaurant_data_dir = './restaurant-data/'  # Directory containing restaurant data files\n",
    "documents = []  # Empty list to store our documents\n",
    "\n",
    "# Loop through all files in the restaurant data directory\n",
    "for filename in os.listdir(restaurant_data_dir):\n",
    "    # Only process .docx files and skip temporary files (those starting with ~)\n",
    "    if filename.endswith('.docx') and not filename.startswith('~'):\n",
    "        file_path = os.path.join(restaurant_data_dir, filename)\n",
    "        try:\n",
    "            # Load the document and add it to our list\n",
    "            doc = load_docx(file_path)\n",
    "            documents.append(doc)\n",
    "            print(f\"Loaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            # If there's an error, print it but continue with other files\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "print(f\"Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "# We split large documents into smaller chunks to make search more effective\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Each chunk will be about 500 characters\n",
    "    chunk_overlap=100  # Chunks will overlap by 100 characters to maintain context\n",
    ")\n",
    "\n",
    "# Apply the splitter to our documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(splits)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Set up embeddings\n",
    "\n",
    "Embeddings are the mathematical representations (vectors) of text. We need to convert our text chunks into these vectors so they can be searched efficiently. For simplicity, we'll use a model from Amazon Bedrock, but in a production environment, you might want to use other embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for embeddings\n",
    "import boto3  # AWS SDK for Python\n",
    "from langchain_aws import BedrockEmbeddings  # For using Amazon Bedrock embeddings\n",
    "\n",
    "# Create a client for Amazon Bedrock\n",
    "# This allows us to communicate with the Bedrock service\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Specify which embedding model to use\n",
    "# Titan is Amazon's embedding model that converts text to vectors\n",
    "bedrock_embedding_model_id = 'amazon.titan-embed-text-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding model object\n",
    "# This will be used to convert our text to vectors\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=bedrock_embedding_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and persist the vector database\n",
    "from langchain_chroma import Chroma  # Chroma is a vector database\n",
    "\n",
    "# Define the directory where we'll save our vector database\n",
    "# This allows us to reuse it later without recreating it\n",
    "persist_directory = './restaurant-vectordb/'\n",
    "\n",
    "# Create the vector database from our document chunks\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,  # Our document chunks\n",
    "    embedding=embedding_model,  # The embedding model to use\n",
    "    persist_directory=persist_directory  # Where to save the database\n",
    ")\n",
    "\n",
    "print(\"Vector database created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Test the Vector Database\n",
    "\n",
    "Now that we've created our vector database, let's test it with a simple query to make sure it works correctly. We'll search for vegetarian options and see what results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple query\n",
    "query = \"What vegetarian options are available?\"  # Our test question\n",
    "results = vectordb.similarity_search(query, k=3)  # Get the top 3 most relevant results\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop 3 results:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")  # Which restaurant this is from\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")  # Show the first 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Set Up LangFuse for Observability\n",
    "\n",
    "**What is LangFuse?**\n",
    "\n",
    "LangFuse is like a dashboard for your AI agent. It helps you see what's happening inside your agent - what questions it's getting, how it's thinking about them, and what answers it's giving. This is incredibly useful for debugging and improving your agent.\n",
    "\n",
    "Now, let's configure LangFuse for observability. You'll need to create a LangFuse account and get your API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create a new project in Langfuse](https://catalog.us-east-1.prod.workshops.aws/workshops/33f099a6-45a2-47d7-9e3c-a23a6568821e/en-US/01-fundamentals/18-agent-observability-and-evaluation#create-a-new-project-in-langfuse)\n",
    "\n",
    "1- Click on Sign-up to create a [langfuse account](https://us.cloud.langfuse.com/) or [Sign-in to an existing account](https://us.cloud.langfuse.com/).\n",
    "\n",
    "![image](image/1-langfuse.png)\n",
    "\n",
    "2- Create a New Organization and enter an orgnization name. Skip the Invite Members. Then create the project. \n",
    "\n",
    "![image](image/project.png)\n",
    "\n",
    "3- Copy and paste the Secret Key, Public Key and Host. Note you can also find the credentials in the Settings -> API Keys page.\n",
    "\n",
    "![api](image/api.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangFuse configuration\n",
    "from langfuse import Langfuse\n",
    "\n",
    "# Replace these with your LangFuse credentials\n",
    "# These keys are like passwords that let your code connect to LangFuse\n",
    "public_key = \"your-public-key\"  # Replace with your public key\n",
    "secret_key = \"your-secret-key\"  # Replace with your secret key\n",
    "\n",
    "# Create a LangFuse client\n",
    "# This is the object we'll use to communicate with LangFuse\n",
    "langfuse = Langfuse(\n",
    "  public_key=public_key,\n",
    "  secret_key=secret_key,\n",
    "  host=\"https://us.cloud.langfuse.com\"  # For US region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we can access the client\n",
    "from langfuse import get_client\n",
    " \n",
    "# Access the client directly\n",
    "langfuse = get_client(public_key=public_key)\n",
    " \n",
    "# Flush all pending observations\n",
    "# This ensures all data is sent to LangFuse\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ  Create a Restaurant Recommendation Agent\n",
    "\n",
    "Now, let's create a Strands Agent that uses our vector database to provide restaurant recommendations. This agent will:\n",
    "1. Receive questions from users about restaurants\n",
    "2. Search our vector database for relevant information\n",
    "3. Generate helpful responses based on the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the agent\n",
    "from strands import Agent  # The main Agent class\n",
    "from strands.models.anthropic import AnthropicModel  # For using Claude model\n",
    "from strands.tools import tool  # For creating tools the agent can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the language model for our agent\n",
    "# We're using Claude 3 Sonnet from Anthropic\n",
    "model = AnthropicModel(\n",
    "    client_args={\n",
    "        \"api_key\": \"your-anthropic-api-key\",  # Replace with your API key\n",
    "    },\n",
    "    max_tokens=1028,  # Maximum response length\n",
    "    model_id=\"claude-3-sonnet-20240229\",  # Which model to use\n",
    "    params={\n",
    "        \"temperature\": 0.3,  # Lower temperature means more consistent, focused responses\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retrieval tool using our vector database\n",
    "# This tool will allow our agent to search for restaurant information\n",
    "\n",
    "@tool\n",
    "def search_restaurants(query):\n",
    "    \"\"\"\n",
    "    Search for restaurant information based on cuisine, dietary preferences, location, or other criteria.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query about restaurants\n",
    "        \n",
    "    Returns:\n",
    "        str: Relevant information about restaurants matching the query\n",
    "    \"\"\"\n",
    "    # Load the persisted vector database\n",
    "    loaded_vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "    \n",
    "    # Perform a similarity search\n",
    "    results = loaded_vectordb.similarity_search(query, k=3)\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = \"\"\n",
    "    for i, doc in enumerate(results):\n",
    "        restaurant_name = os.path.basename(doc.metadata['source']).replace('.docx', '')\n",
    "        formatted_results += f\"Restaurant: {restaurant_name}\\n\"\n",
    "        formatted_results += f\"Information: {doc.page_content}\\n\\n\"\n",
    "    \n",
    "    return formatted_results if formatted_results else \"No relevant information found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the restaurant recommendation agent\n",
    "import uuid  # For generating unique IDs\n",
    "\n",
    "restaurant_agent = Agent(\n",
    "    name = \"Restaurant Recommendation Agent\",\n",
    "    model=model,\n",
    "    tools=[search_restaurants],  # Give the agent access to our search tool\n",
    "    system_prompt=\"\"\"You are a helpful restaurant recommendation assistant. \n",
    "    Use the search_restaurants tool to find information about restaurants based on user queries.\n",
    "    Provide detailed recommendations based on the search results.\n",
    "    If asked about restaurants that aren't in the database, politely explain that you can only provide information about restaurants in your database.\n",
    "    Always be friendly, helpful, and concise in your responses.\n",
    "    \"\"\",\n",
    "    record_direct_tool_call = True,  # Record when tools are used\n",
    "    trace_attributes={\n",
    "        \"session.id\": str(uuid.uuid4()),  # Generate a unique session ID\n",
    "        \"user.id\": \"user-email-example@domain.com\",  # Example user ID\n",
    "        \"langfuse.tags\": [\n",
    "            \"Agent-SDK-Example\",\n",
    "            \"Strands-Project-Demo\",\n",
    "            \"Observability-Tutorial\"\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ  Test the Agent with Tracing\n",
    "\n",
    "Now let's test our agent with a simple query and see how it performs. The agent will use the search tool to find relevant information and then generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a simple query\n",
    "response = restaurant_agent(\"I'm looking for a restaurant with good vegetarian options. Any recommendations?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Review the traces\n",
    "\n",
    "After running the agent, you can review the traces in LangFuse:\n",
    "\n",
    "1. Go to the tracing menu in your LangFuse project\n",
    "2. Select the trace you want to view\n",
    "3. Examine how the agent processed the request, what tools it used, and what response it generated\n",
    "\n",
    "This gives you visibility into how your agent is working and helps you identify any issues or areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up RAGAS for Evaluation\n",
    "\n",
    "Now, let's use RAGAS to evaluate the quality of our agent's responses. RAGAS (Retrieval Augmented Generation Assessment) is a framework for evaluating RAG systems.\n",
    "\n",
    "### What is RAGAS?\n",
    "\n",
    "RAGAS helps us measure how well our agent is performing by looking at different aspects of its responses:\n",
    "- Is the information accurate?\n",
    "- Is it relevant to the user's question?\n",
    "- Is it using the right tools?\n",
    "- Is it communicating in a friendly way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RAGAS libraries\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# Set up the evaluator LLM (we'll use the same model as our agent)\n",
    "evaluator_llm = LangchainLLMWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RAGAS Metrics\n",
    "\n",
    "We'll define several metrics to evaluate different aspects of our agent's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "# Metric to check if the agent fulfills all user requests\n",
    "request_completeness = AspectCritic(\n",
    "    name=\"Request Completeness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent completely fulfills all the user requests with no omissions. \"\n",
    "        \"otherwise, return 0.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Metric to assess if the AI's communication aligns with the desired brand voice\n",
    "brand_tone = AspectCritic(\n",
    "    name=\"Brand Voice Metric\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the AI's communication is friendly, approachable, helpful, clear, and concise; \"\n",
    "        \"otherwise, return 0.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Tool usage effectiveness metric\n",
    "tool_usage_effectiveness = AspectCritic(\n",
    "    name=\"Tool Usage Effectiveness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent appropriately used available tools to fulfill the user's request \"\n",
    "        \"(such as using retrieve for menu questions and current_time for time questions). \"\n",
    "        \"Return 0 if the agent failed to use appropriate tools or used unnecessary tools.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Tool selection appropriateness metric\n",
    "tool_selection_appropriateness = AspectCritic(\n",
    "    name=\"Tool Selection Appropriateness\",\n",
    "    llm=evaluator_llm,\n",
    "    definition=(\n",
    "        \"Return 1 if the agent selected the most appropriate tools for the task. \"\n",
    "        \"Return 0 if better tool choices were available or if unnecessary tools were selected.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a rubric score metric to evaluate how the agent handles situations where requested items aren't available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import RubricsScore\n",
    "\n",
    "# Define a rubric for evaluating recommendations\n",
    "rubrics = {\n",
    "    \"score-1_description\": (\n",
    "        \"\"\"The item requested by the customer is not present in the menu and no \n",
    "        recommendations were made.\"\"\"\n",
    "    ),\n",
    "    \"score0_description\": (\n",
    "        \"Either the item requested by the customer is present in the menu, \"\n",
    "        \"or the conversation does not include any \"\n",
    "        \"food or menu inquiry (e.g., booking, cancellation). \"\n",
    "        \"This score applies regardless of whether any recommendation was \"\n",
    "        \"provided.\"\n",
    "    ),\n",
    "    \"score1_description\": (\n",
    "        \"The item requested by the customer is not present in the menu \"\n",
    "        \"and a recommendation was provided.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Create the recommendations metric\n",
    "recommendations = RubricsScore(rubrics=rubrics, llm=evaluator_llm, name=\"Recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define metrics to evaluate the RAG (Retrieval-Augmented Generation) aspects of our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextRelevance, ResponseGroundedness \n",
    "\n",
    "# Context relevance measures how well the retrieved contexts address the user's query\n",
    "context_relevance = ContextRelevance(llm=evaluator_llm)\n",
    "\n",
    "# Response groundedness determines if the response is supported by the provided contexts\n",
    "response_groundedness = ResponseGroundedness(llm=evaluator_llm)\n",
    "\n",
    "metrics=[context_relevance, response_groundedness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Agent and Send Results to LangFuse\n",
    "\n",
    "Now we'll create functions to evaluate our agent and send the results back to LangFuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from ragas.dataset_schema import (\n",
    "    SingleTurnSample,\n",
    "    MultiTurnSample,\n",
    "    EvaluationDataset\n",
    ")\n",
    "from ragas import evaluate\n",
    "\n",
    "# Function to evaluate agent responses\n",
    "def evaluate_agent_response(query, response, context):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's response using RAGAS metrics.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's query\n",
    "        response (str): The agent's response\n",
    "        context (str): The context used to generate the response\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation scores\n",
    "    \"\"\"\n",
    "    # Create a dataset for evaluation\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query,\n",
    "        response=response,\n",
    "        retrieved_contexts=[context]\n",
    "    )\n",
    "    dataset = EvaluationDataset(samples=[sample])\n",
    "    \n",
    "    # Run evaluation with RAGAS metrics\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[\n",
    "            context_relevance,\n",
    "            response_groundedness\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send evaluation scores to LangFuse\n",
    "def send_evaluation_to_langfuse(trace_id, evaluation_result):\n",
    "    \"\"\"\n",
    "    Send evaluation scores to LangFuse.\n",
    "    \n",
    "    Args:\n",
    "        trace_id (str): The trace ID from LangFuse\n",
    "        evaluation_result (dict): The evaluation results from RAGAS\n",
    "    \"\"\"\n",
    "    # Convert evaluation result to pandas DataFrame\n",
    "    eval_df = evaluation_result.to_pandas()\n",
    "    \n",
    "    # Extract scores from evaluation result\n",
    "    scores = {}\n",
    "    for column in eval_df.columns:\n",
    "        if column not in ['user_input', 'response', 'retrieved_contexts']:\n",
    "            try:\n",
    "                scores[column] = float(eval_df[column].iloc[0])\n",
    "            except:\n",
    "                scores[column] = 0.0\n",
    "    \n",
    "    # Calculate average score\n",
    "    avg_score = sum(scores.values()) / len(scores) if scores else 0\n",
    "    \n",
    "    # Send scores to LangFuse\n",
    "    langfuse.score(\n",
    "        trace_id=trace_id,\n",
    "        name=\"ragas_evaluation\",\n",
    "        value=avg_score,\n",
    "        comment=\"RAGAS evaluation scores\",\n",
    "        metadata=scores\n",
    "    )\n",
    "    \n",
    "    print(f\"Evaluation scores sent to LangFuse for trace {trace_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a complete example with tracing and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a complete example with tracing and evaluation\n",
    "query = \"I need a restaurant for a business dinner with clients who prefer seafood. What do you recommend?\"\n",
    "\n",
    "# Start a new trace in LangFuse\n",
    "trace = langfuse.trace(name=\"restaurant_recommendation\")\n",
    "\n",
    "# Get context\n",
    "context = search_restaurants(query)\n",
    "\n",
    "# Log the context retrieval\n",
    "context_span = trace.span(\n",
    "    name=\"context_retrieval\",\n",
    "    input={\"query\": query},\n",
    "    output={\"context\": context}\n",
    ")\n",
    "\n",
    "# Get agent response\n",
    "response = restaurant_agent(\n",
    "    query,\n",
    "    metadata={\"langfuse_trace_id\": trace.id}\n",
    ")\n",
    "\n",
    "# Log the agent response\n",
    "response_span = trace.span(\n",
    "    name=\"agent_response\",\n",
    "    input={\"query\": query},\n",
    "    output={\"response\": response}\n",
    ")\n",
    "\n",
    "# Evaluate the response\n",
    "evaluation_result = evaluate_agent_response(query, response, context)\n",
    "\n",
    "# Send evaluation to LangFuse\n",
    "send_evaluation_to_langfuse(trace.id, evaluation_result)\n",
    "\n",
    "# End the trace\n",
    "trace.end()\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nAgent Response:\")\n",
    "print(response)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(evaluation_result.to_pandas())\n",
    "print(f\"\\nLangFuse Trace ID: {trace.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Create a local vector database from restaurant data files\n",
    "2. Build a restaurant recommendation agent using Strands Agent\n",
    "3. Set up LangFuse for observability and tracing\n",
    "4. Use RAGAS to evaluate the quality of agent responses\n",
    "5. Send evaluation results back to LangFuse\n",
    "\n",
    "This approach provides a comprehensive framework for building, monitoring, and evaluating AI agents without requiring complex AWS infrastructure deployment.\n",
    "\n",
    "You can view your traces and evaluation results in the LangFuse dashboard to gain insights into your agent's performance and identify areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

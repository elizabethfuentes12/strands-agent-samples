{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde193fa",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Multimodal Processing with Local Vector Memory\n",
    "\n",
    "This notebook demonstrates multimodal content processing with persistent memory using [FAISS](https://github.com/facebookresearch/faiss) for local vector storage. You'll build an agent that remembers previous conversations and content across sessions using [Mem0](https://mem0.ai/) integration.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Implement persistent memory with FAISS vector storage\n",
    "- Maintain conversation context across multiple sessions\n",
    "- Process multimodal content with memory-enhanced responses\n",
    "- Use semantic search for relevant context retrieval\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with [Amazon Bedrock access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html)\n",
    "- Claude 3.5 Sonnet model enabled in Bedrock\n",
    "- Python packages: `mem0ai`, `faiss-cpu`, `opensearch-py`\n",
    "\n",
    "## Memory Architecture\n",
    "\n",
    "This notebook uses FAISS for local development. For production deployments, consider [Amazon S3 Vectors](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-vectors.html) as shown in the S3 vectors notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d585e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mem0ai\n",
    "!pip install opensearch-py\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from strands.models import BedrockModel\n",
    "from strands import Agent\n",
    "from strands_tools import image_reader, file_read, mem0_memory,use_llm\n",
    "from video_reader import video_reader\n",
    "from strands.tools import tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4559fc",
   "metadata": {},
   "source": [
    "## ðŸ¤– Agent Configuration with FAISS Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt with memory capabilities instruction\n",
    "MULTIMODAL_SYSTEM_PROMPT = \"\"\" You are a helpful assistant that can process documents, images, and videos. \n",
    "Analyze their contents and provide relevant information. You have memory capabilities and can remember previous interactions.\n",
    "\n",
    "You can:\n",
    "\n",
    "1. For PNG, JPEG/JPG, GIF, or WebP formats use image_reader to process file\n",
    "2. For PDF, csv, docx, xls or xlsx formats use file_read to process file  \n",
    "3. For MP4, MOV, AVI, MKV, WebM formats use video_reader to process file\n",
    "4. Just deliver the answer\n",
    "\n",
    "memory capabilities:\n",
    "- Store new information using mem0_memory tool (action=\"store\")\n",
    "- Retrieve relevant memories (action=\"retrieve\")\n",
    "- List all memories (action=\"list\")\n",
    "- Provide personalized responses\n",
    "\n",
    "Key Rules:\n",
    "- Always include user_id={USER_ID} in tool calls\n",
    "- Be conversational and natural in responses\n",
    "- Format output clearly\n",
    "- Acknowledge stored information\n",
    "- Only share relevant information\n",
    "- Politely indicate when information is unavailable\n",
    "\n",
    "\n",
    "When displaying responses:\n",
    "- Format answers data in a human-readable way\n",
    "- Highlight important information\n",
    "- Handle errors appropriately\n",
    "- Convert technical terms to user-friendly language\n",
    "- Always reply in the original user language\n",
    "- Reference relevant past interactions when appropriate\n",
    "\n",
    "Always reply in the original user language.\n",
    "\"\"\"\n",
    "\n",
    "# Configure AWS session for Bedrock access\n",
    "session = boto3.Session(region_name='us-west-2')\n",
    "\n",
    "# Initialize Bedrock model for inference\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    #model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    boto_session=session,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "# Create enhanced agent with memory capabilities\n",
    "multimodal_agent = Agent(\n",
    "    system_prompt=MULTIMODAL_SYSTEM_PROMPT,\n",
    "    tools=[image_reader, file_read, video_reader, mem0_memory,use_llm],\n",
    "    model=bedrock_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3931c4",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Initialize some demo memories to showcase functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43601dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ID = \"eli_abc\"  # Generate a unique user ID\n",
    "content = \"\"\"Hello, my name is Elizabeth, but they call me Eli. I'm a developer advocate at AWS, and I want to understand what's in images, videos, and documents to improve my day-to-day work.\"\"\" \n",
    "multimodal_agent.tool.mem0_memory(action=\"store\", content=content, user_id=USER_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6f952",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Usage Examples with Memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Image analysis with memory storage\n",
    "print(\"=== ðŸ“¸ IMAGE ANALYSIS WITH MEMORY ===\")\n",
    "image_result = multimodal_agent(f\"Analyze the image data-sample/diagram.jpg in detail and describe everything you observe. Remember this information for later. USER_ID\")\n",
    "print(image_result)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text content from response\n",
    "image_result.message['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c29369",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve memories\n",
    "retrieved_memories = multimodal_agent.tool.mem0_memory(\n",
    "    action=\"retrieve\", query=\"What services are in the image?\", user_id=USER_ID\n",
    ")\n",
    "print(\"Retrieved Memories:\", retrieved_memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b93095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List all stored memories\n",
    "print(\"All Stored Memories:\")\n",
    "all_memories = multimodal_agent.tool.mem0_memory(\n",
    "    action=\"list\", user_id=USER_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45875c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text content from response\n",
    "print(image_result.message['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Video analysis with memory storage\n",
    "print(\"=== ðŸŽ¬ VIDEO ANALYSIS WITH MEMORY ===\")\n",
    "video_result = multimodal_agent(\"Analyze the video data-sample/moderation-video.mp4 and describe in detail the actions and scenes you observe. Store this information in your memory.\")\n",
    "print(video_result)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec38e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text content from response\n",
    "video_result.message['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30275dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Document analysis with memory storage\n",
    "print(\"=== ðŸ“„ DOCUMENT ANALYSIS WITH MEMORY ===\")\n",
    "doc_result = multimodal_agent(\"Summarize as json the content of the document data-sample/Welcome-Strands-Agents-SDK.pdf and store this information in your memory.\")\n",
    "print(doc_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b24ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text content from response\n",
    "doc_result.message['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Testing memory recall across multiple media types\n",
    "print(\"=== ðŸ§  MEMORY RECALL TEST ===\")\n",
    "memory_result = multimodal_agent(\"What do you remember about the image, video, and document I showed you earlier?\")\n",
    "print(memory_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text content from response\n",
    "memory_result.message['content'][0]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
